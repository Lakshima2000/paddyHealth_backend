{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in ./venv/lib/python3.12/site-packages (6.3.1)\n",
      "Requirement already satisfied: regex in ./venv/lib/python3.12/site-packages (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.12/site-packages (from ftfy) (0.2.13)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/f0/vdhww2m16hvdbbnjjqfjwqhm0000gp/T/pip-req-build-spq63nyh\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/f0/vdhww2m16hvdbbnjjqfjwqhm0000gp/T/pip-req-build-spq63nyh\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in ./venv/lib/python3.12/site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from clip==1.0) (25.0)\n",
      "Requirement already satisfied: regex in ./venv/lib/python3.12/site-packages (from clip==1.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.12/site-packages (from clip==1.0) (2.7.0)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.12/site-packages (from clip==1.0) (0.22.0)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.12/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch->clip==1.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.12/site-packages (from torch->clip==1.0) (4.13.2)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch->clip==1.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.12/site-packages (from torch->clip==1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch->clip==1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch->clip==1.0) (2025.5.1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from torchvision->clip==1.0) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.12/site-packages (from torchvision->clip==1.0) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 1. Setup device\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Setup device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 2. Load CLIP model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# 3. Custom Dataset Class\n",
    "class RiceLeafDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform or preprocess\n",
    "\n",
    "        self.classes = [cls for cls in sorted(os.listdir(root_dir)) \n",
    "                        if os.path.isdir(os.path.join(root_dir, cls)) and not cls.startswith('.')]\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "        self.images = self._load_images()\n",
    "\n",
    "    def _load_images(self):\n",
    "        images = []\n",
    "        for cls in self.classes:\n",
    "            cls_path = os.path.join(self.root_dir, cls)\n",
    "            for img_name in os.listdir(cls_path):\n",
    "                if not img_name.startswith('.') and img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(cls_path, img_name)\n",
    "                    try:\n",
    "                        Image.open(img_path).convert(\"RGB\")\n",
    "                        images.append((img_path, self.class_to_idx[cls]))\n",
    "                    except:\n",
    "                        print(f\"Skipping corrupted image: {img_path}\")\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.images[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 4. Prepare Data\n",
    "dataset = RiceLeafDataset('/Users/tharindua/Downloads/Rice_Leaf_AUG', transform=preprocess)\n",
    "print(\"Class to index mapping:\", dataset.class_to_idx)\n",
    "\n",
    "train_idx, val_idx = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# 5. Fine-Tune Model\n",
    "class FineTunedCLIP(torch.nn.Module):\n",
    "    def __init__(self, clip_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "        self.classifier = torch.nn.Linear(512, num_classes)  # CLIP ViT-B/32 outputs 512-dim\n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():  # Freeze CLIP backbone during initial training\n",
    "            image_features = self.clip.encode_image(images)\n",
    "        return self.classifier(image_features.float())\n",
    "\n",
    "# 6. Initialize model and freeze CLIP\n",
    "num_classes = len(dataset.classes)\n",
    "ft_model = FineTunedCLIP(model, num_classes).to(device)\n",
    "\n",
    "# Optionally unfreeze CLIP after some epochs for fine-tuning\n",
    "for param in ft_model.clip.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 7. Loss & Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(ft_model.classifier.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# 8. Train and Validate Functions\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "\n",
    "    for images, labels in tqdm(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "# 9. Training Loop\n",
    "best_val_acc = 0\n",
    "for epoch in range(20):\n",
    "    train_loss, train_acc = train_epoch(ft_model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = validate(ft_model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(ft_model.state_dict(), \"best_clip_rice_model.pth\")\n",
    "        print(\"✅ New best model saved!\")\n",
    "\n",
    "# 10. Load and Evaluate\n",
    "ft_model.load_state_dict(torch.load(\"best_clip_rice_model.pth\"))\n",
    "_, final_acc = validate(ft_model, val_loader, criterion, device)\n",
    "print(f\"📊 Final Validation Accuracy: {final_acc:.4f}\")\n",
    "\n",
    "# 11. Single Image Prediction Function\n",
    "def predict_image(image_path, model, preprocess, class_mapping):\n",
    "    model.eval()\n",
    "    image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        pred = output.argmax(1).item()\n",
    "    return class_mapping[pred]\n",
    "\n",
    "# Example usage:\n",
    "class_mapping = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "image_path = \"/Users/tharindua/Downloads/sample_leaf.jpg\"  # Replace with test image\n",
    "predicted_class = predict_image(image_path, ft_model, preprocess, class_mapping)\n",
    "print(f\"🧠 Predicted class: {predicted_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
